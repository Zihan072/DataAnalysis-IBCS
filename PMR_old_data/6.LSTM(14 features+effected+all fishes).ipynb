{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39455e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from torch.autograd import Variable \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob, os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee739f1",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169606a3",
   "metadata": {},
   "source": [
    "## 1.1 load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d113c31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Pre: Q1</th>\n",
       "      <th>Pre: Q3</th>\n",
       "      <th>L1: Q1</th>\n",
       "      <th>L1: Q3</th>\n",
       "      <th>E1: Q1</th>\n",
       "      <th>E1: Q3</th>\n",
       "      <th>E2: Q1</th>\n",
       "      <th>E2: Q3</th>\n",
       "      <th>E3: Q1</th>\n",
       "      <th>E3: Q3</th>\n",
       "      <th>R1: Q1</th>\n",
       "      <th>R1: Q3</th>\n",
       "      <th>R2: Q1</th>\n",
       "      <th>R2: Q3</th>\n",
       "      <th>action mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C27_00</td>\n",
       "      <td>0.028764</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>0.298368</td>\n",
       "      <td>0.050132</td>\n",
       "      <td>0.278168</td>\n",
       "      <td>0.037783</td>\n",
       "      <td>0.130060</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>0.034696</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>0.034636</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C27_01</td>\n",
       "      <td>0.034007</td>\n",
       "      <td>0.037103</td>\n",
       "      <td>0.036158</td>\n",
       "      <td>0.292708</td>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.241567</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>0.038092</td>\n",
       "      <td>0.033519</td>\n",
       "      <td>0.034894</td>\n",
       "      <td>0.034084</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.033932</td>\n",
       "      <td>0.036275</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C27_02</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.070414</td>\n",
       "      <td>0.015166</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>0.015186</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.012020</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C27_03</td>\n",
       "      <td>0.032434</td>\n",
       "      <td>0.044575</td>\n",
       "      <td>0.031910</td>\n",
       "      <td>0.582665</td>\n",
       "      <td>0.042110</td>\n",
       "      <td>0.161612</td>\n",
       "      <td>0.020669</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.018032</td>\n",
       "      <td>0.017628</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.019698</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C27_04</td>\n",
       "      <td>0.026860</td>\n",
       "      <td>0.051772</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>0.298368</td>\n",
       "      <td>0.050132</td>\n",
       "      <td>0.278168</td>\n",
       "      <td>0.037783</td>\n",
       "      <td>0.130060</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>0.034696</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>0.034636</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>WT_Control_39_06</td>\n",
       "      <td>0.029761</td>\n",
       "      <td>0.030939</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>0.043304</td>\n",
       "      <td>0.032474</td>\n",
       "      <td>0.087818</td>\n",
       "      <td>0.028630</td>\n",
       "      <td>0.030874</td>\n",
       "      <td>0.028339</td>\n",
       "      <td>0.029058</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.029104</td>\n",
       "      <td>0.033801</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>WT_Control_39_07</td>\n",
       "      <td>0.025772</td>\n",
       "      <td>0.028487</td>\n",
       "      <td>0.139233</td>\n",
       "      <td>0.289870</td>\n",
       "      <td>0.053855</td>\n",
       "      <td>0.103895</td>\n",
       "      <td>0.027061</td>\n",
       "      <td>0.038182</td>\n",
       "      <td>0.025958</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.025788</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.027941</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>WT_Control_39_08</td>\n",
       "      <td>0.032074</td>\n",
       "      <td>0.033806</td>\n",
       "      <td>0.054066</td>\n",
       "      <td>0.285480</td>\n",
       "      <td>0.036312</td>\n",
       "      <td>0.105165</td>\n",
       "      <td>0.032086</td>\n",
       "      <td>0.037296</td>\n",
       "      <td>0.030585</td>\n",
       "      <td>0.031808</td>\n",
       "      <td>0.030598</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.030454</td>\n",
       "      <td>0.031774</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>WT_Control_39_09</td>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.019578</td>\n",
       "      <td>0.030485</td>\n",
       "      <td>0.098099</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.020729</td>\n",
       "      <td>0.035798</td>\n",
       "      <td>0.020431</td>\n",
       "      <td>0.020886</td>\n",
       "      <td>0.020524</td>\n",
       "      <td>0.020836</td>\n",
       "      <td>0.020473</td>\n",
       "      <td>0.020871</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>WT_Control_39_10</td>\n",
       "      <td>0.020306</td>\n",
       "      <td>0.022405</td>\n",
       "      <td>0.030565</td>\n",
       "      <td>0.089496</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.109892</td>\n",
       "      <td>0.020938</td>\n",
       "      <td>0.021608</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.020816</td>\n",
       "      <td>0.021451</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.021482</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4803 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index   Pre: Q1   Pre: Q3    L1: Q1    L1: Q3    E1: Q1  \\\n",
       "0               C27_00  0.028764  0.068646  0.028082  0.298368  0.050132   \n",
       "1               C27_01  0.034007  0.037103  0.036158  0.292708  0.046303   \n",
       "2               C27_02  0.021472  0.070414  0.015166  0.017657  0.012417   \n",
       "3               C27_03  0.032434  0.044575  0.031910  0.582665  0.042110   \n",
       "4               C27_04  0.026860  0.051772  0.028082  0.298368  0.050132   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "4798  WT_Control_39_06  0.029761  0.030939  0.032357  0.043304  0.032474   \n",
       "4799  WT_Control_39_07  0.025772  0.028487  0.139233  0.289870  0.053855   \n",
       "4800  WT_Control_39_08  0.032074  0.033806  0.054066  0.285480  0.036312   \n",
       "4801  WT_Control_39_09  0.019190  0.019578  0.030485  0.098099  0.031112   \n",
       "4802  WT_Control_39_10  0.020306  0.022405  0.030565  0.089496  0.027522   \n",
       "\n",
       "        E1: Q3    E2: Q1    E2: Q3    E3: Q1    E3: Q3    R1: Q1    R1: Q3  \\\n",
       "0     0.278168  0.037783  0.130060  0.031555  0.034696  0.031200  0.033734   \n",
       "1     0.241567  0.036107  0.038092  0.033519  0.034894  0.034084  0.035500   \n",
       "2     0.015186  0.010722  0.012020  0.010183  0.010478  0.010575  0.010774   \n",
       "3     0.161612  0.020669  0.040788  0.017505  0.018032  0.017628  0.018242   \n",
       "4     0.278168  0.037783  0.130060  0.031555  0.034696  0.031200  0.033734   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4798  0.087818  0.028630  0.030874  0.028339  0.029058  0.028269  0.029104   \n",
       "4799  0.103895  0.027061  0.038182  0.025958  0.027907  0.025788  0.027965   \n",
       "4800  0.105165  0.032086  0.037296  0.030585  0.031808  0.030598  0.031539   \n",
       "4801  0.065068  0.020729  0.035798  0.020431  0.020886  0.020524  0.020836   \n",
       "4802  0.109892  0.020938  0.021608  0.020817  0.021400  0.020816  0.021451   \n",
       "\n",
       "        R2: Q1    R2: Q3                  action mode  \n",
       "0     0.031431  0.034636  GABAA allosteric antagonist  \n",
       "1     0.033932  0.036275  GABAA allosteric antagonist  \n",
       "2     0.009926  0.010305  GABAA allosteric antagonist  \n",
       "3     0.019698  0.020793  GABAA allosteric antagonist  \n",
       "4     0.031431  0.034636  GABAA allosteric antagonist  \n",
       "...        ...       ...                          ...  \n",
       "4798  0.033801  0.035754                   WT_control  \n",
       "4799  0.025757  0.027941                   WT_control  \n",
       "4800  0.030454  0.031774                   WT_control  \n",
       "4801  0.020473  0.020871                   WT_control  \n",
       "4802  0.020759  0.021482                   WT_control  \n",
       "\n",
       "[4803 rows x 16 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./C5-C129/all_fished_with_labels/effected_compounds_fishes_labeled.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71456cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pre: Q1</th>\n",
       "      <th>Pre: Q3</th>\n",
       "      <th>L1: Q1</th>\n",
       "      <th>L1: Q3</th>\n",
       "      <th>E1: Q1</th>\n",
       "      <th>E1: Q3</th>\n",
       "      <th>E2: Q1</th>\n",
       "      <th>E2: Q3</th>\n",
       "      <th>E3: Q1</th>\n",
       "      <th>E3: Q3</th>\n",
       "      <th>R1: Q1</th>\n",
       "      <th>R1: Q3</th>\n",
       "      <th>R2: Q1</th>\n",
       "      <th>R2: Q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028764</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>0.298368</td>\n",
       "      <td>0.050132</td>\n",
       "      <td>0.278168</td>\n",
       "      <td>0.037783</td>\n",
       "      <td>0.130060</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>0.034696</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>0.034636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034007</td>\n",
       "      <td>0.037103</td>\n",
       "      <td>0.036158</td>\n",
       "      <td>0.292708</td>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.241567</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>0.038092</td>\n",
       "      <td>0.033519</td>\n",
       "      <td>0.034894</td>\n",
       "      <td>0.034084</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.033932</td>\n",
       "      <td>0.036275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.070414</td>\n",
       "      <td>0.015166</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>0.015186</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.012020</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>0.010305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.032434</td>\n",
       "      <td>0.044575</td>\n",
       "      <td>0.031910</td>\n",
       "      <td>0.582665</td>\n",
       "      <td>0.042110</td>\n",
       "      <td>0.161612</td>\n",
       "      <td>0.020669</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.018032</td>\n",
       "      <td>0.017628</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.019698</td>\n",
       "      <td>0.020793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026860</td>\n",
       "      <td>0.051772</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>0.298368</td>\n",
       "      <td>0.050132</td>\n",
       "      <td>0.278168</td>\n",
       "      <td>0.037783</td>\n",
       "      <td>0.130060</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>0.034696</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>0.034636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>0.029761</td>\n",
       "      <td>0.030939</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>0.043304</td>\n",
       "      <td>0.032474</td>\n",
       "      <td>0.087818</td>\n",
       "      <td>0.028630</td>\n",
       "      <td>0.030874</td>\n",
       "      <td>0.028339</td>\n",
       "      <td>0.029058</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.029104</td>\n",
       "      <td>0.033801</td>\n",
       "      <td>0.035754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>0.025772</td>\n",
       "      <td>0.028487</td>\n",
       "      <td>0.139233</td>\n",
       "      <td>0.289870</td>\n",
       "      <td>0.053855</td>\n",
       "      <td>0.103895</td>\n",
       "      <td>0.027061</td>\n",
       "      <td>0.038182</td>\n",
       "      <td>0.025958</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.025788</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.027941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>0.032074</td>\n",
       "      <td>0.033806</td>\n",
       "      <td>0.054066</td>\n",
       "      <td>0.285480</td>\n",
       "      <td>0.036312</td>\n",
       "      <td>0.105165</td>\n",
       "      <td>0.032086</td>\n",
       "      <td>0.037296</td>\n",
       "      <td>0.030585</td>\n",
       "      <td>0.031808</td>\n",
       "      <td>0.030598</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.030454</td>\n",
       "      <td>0.031774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.019578</td>\n",
       "      <td>0.030485</td>\n",
       "      <td>0.098099</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.020729</td>\n",
       "      <td>0.035798</td>\n",
       "      <td>0.020431</td>\n",
       "      <td>0.020886</td>\n",
       "      <td>0.020524</td>\n",
       "      <td>0.020836</td>\n",
       "      <td>0.020473</td>\n",
       "      <td>0.020871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>0.020306</td>\n",
       "      <td>0.022405</td>\n",
       "      <td>0.030565</td>\n",
       "      <td>0.089496</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.109892</td>\n",
       "      <td>0.020938</td>\n",
       "      <td>0.021608</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.020816</td>\n",
       "      <td>0.021451</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.021482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4803 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pre: Q1   Pre: Q3    L1: Q1    L1: Q3    E1: Q1    E1: Q3    E2: Q1  \\\n",
       "0     0.028764  0.068646  0.028082  0.298368  0.050132  0.278168  0.037783   \n",
       "1     0.034007  0.037103  0.036158  0.292708  0.046303  0.241567  0.036107   \n",
       "2     0.021472  0.070414  0.015166  0.017657  0.012417  0.015186  0.010722   \n",
       "3     0.032434  0.044575  0.031910  0.582665  0.042110  0.161612  0.020669   \n",
       "4     0.026860  0.051772  0.028082  0.298368  0.050132  0.278168  0.037783   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4798  0.029761  0.030939  0.032357  0.043304  0.032474  0.087818  0.028630   \n",
       "4799  0.025772  0.028487  0.139233  0.289870  0.053855  0.103895  0.027061   \n",
       "4800  0.032074  0.033806  0.054066  0.285480  0.036312  0.105165  0.032086   \n",
       "4801  0.019190  0.019578  0.030485  0.098099  0.031112  0.065068  0.020729   \n",
       "4802  0.020306  0.022405  0.030565  0.089496  0.027522  0.109892  0.020938   \n",
       "\n",
       "        E2: Q3    E3: Q1    E3: Q3    R1: Q1    R1: Q3    R2: Q1    R2: Q3  \n",
       "0     0.130060  0.031555  0.034696  0.031200  0.033734  0.031431  0.034636  \n",
       "1     0.038092  0.033519  0.034894  0.034084  0.035500  0.033932  0.036275  \n",
       "2     0.012020  0.010183  0.010478  0.010575  0.010774  0.009926  0.010305  \n",
       "3     0.040788  0.017505  0.018032  0.017628  0.018242  0.019698  0.020793  \n",
       "4     0.130060  0.031555  0.034696  0.031200  0.033734  0.031431  0.034636  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4798  0.030874  0.028339  0.029058  0.028269  0.029104  0.033801  0.035754  \n",
       "4799  0.038182  0.025958  0.027907  0.025788  0.027965  0.025757  0.027941  \n",
       "4800  0.037296  0.030585  0.031808  0.030598  0.031539  0.030454  0.031774  \n",
       "4801  0.035798  0.020431  0.020886  0.020524  0.020836  0.020473  0.020871  \n",
       "4802  0.021608  0.020817  0.021400  0.020816  0.021451  0.020759  0.021482  \n",
       "\n",
       "[4803 rows x 14 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:, 1:-1]\n",
    "#data_array = \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00fcb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='action mode', ylabel='count'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.countplot(x='action mode', data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7722beae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(data['action mode'])\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62568291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GABAA allosteric antagonist', 'GABAA pore blocker',\n",
       "       'TRPV agonist', 'WT_control'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2idx = {\n",
    "    'GABAA allosteric antagonist':0,\n",
    "    'GABAA pore blocker':1,\n",
    "    'TRPV agonist':2,\n",
    "    'WT_control':3,\n",
    "}\n",
    "\n",
    "idx2class = {v: k for k, v in class2idx.items()}\n",
    "\n",
    "data['action mode'].replace(class2idx, inplace=True)\n",
    "\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc13ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.array(X))\n",
    "Y = torch.tensor(np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ecfe1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4803, 14])\n",
      "torch.Size([4803])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6baecac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 14])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:4000, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c79445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = Y.reshape(-1, 1)\n",
    "# Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964f36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 4000 for training\n",
    "\n",
    "X_train = X[:4000, :]\n",
    "X_test = X[4000:, :]\n",
    "\n",
    "y_train = Y[:4000]\n",
    "y_test = Y[4000:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e7534d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape torch.Size([4000, 14]) torch.Size([4000])\n",
      "Testing Shape torch.Size([803, 14]) torch.Size([803])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Shape\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Shape\", X_test.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b89c6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping to rows, timestamps, features\n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "\n",
    "X_test_tensors_final = torch.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c63aa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape torch.Size([4000, 14, 1]) torch.Size([4000])\n",
      "Testing Shape torch.Size([803, 14, 1]) torch.Size([803])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors_final = X_train_tensors_final.transpose(1, 2)\n",
    "X_test_tensors_final = X_test_tensors_final.transpose(1, 2)\n",
    "\n",
    "# In computer memory: there is no tensor / 2d Matrix or anything, there is only array\n",
    "# In order to let the computer understand that this is a matrix, we need to tell the computer how to access the elements in the array.\n",
    "# For example: a matrix size (20, 50) is array that has 1000 elements but we tell the computer that it has 20 rows.\n",
    "\n",
    "# So tranpose doesn't change the order of the elements in the array, but only the information of row and collumn\n",
    "# Reshape in some cases can change the order of the elements in the array\n",
    "# The computation can only be done when the order of the array matches the information of the row/collumns\n",
    "# So Reshape is sometimes necessary to avoid errors\n",
    "print(\"Training Shape\", X_train_tensors_final.shape, y_train.shape)\n",
    "print(\"Testing Shape\", X_test_tensors_final.shape, y_test.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aea37d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the batch has 4000 sequences, each sequence has 14 timesteps, each timestep has 1 feature\n",
    "# the \"label batch\"\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, hidden_size) #fully connected 1\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward function of the model.\n",
    "    We should imagine the size of the tensors in the forward function\n",
    "    * batch_size: the number of samples to be computed at the same time\n",
    "    when we have multiple samples, the loss is the sum or average of the loss for each sample.\n",
    "    batch_size is generally the same throughout one forward pass.\n",
    "    for example: batch_size of the input is 1000 then batch size  of the output is also 1000.\n",
    "    -> batch_size is rarely changed during one training step.\n",
    "    \n",
    "    * time: how many steps are there for each sample in the data, how long the data is\n",
    "    - note 1: in your dataset all samples have the same length\n",
    "    - note 2: in some cases the samples have different lengths > more difficult \n",
    "    \n",
    "    * hidden_size: how many features the data has for every time step\n",
    "    \"\"\"\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        inputs: x: torch.Tensor size: [bsz * time * input_hidden_size]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        # the initial states of the recurrent net: each layer has 1 state h_0 and 1 state c_0\n",
    "        # each state h_0 has bsz * hidden_size\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device) #hidden state\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        #out = self.relu(hn)\n",
    "        #out = self.fc_1(out) #first Dense\n",
    "        #out = self.relu(out) #relu\n",
    "        out = self.fc(hn) #Final Output\n",
    "        return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e02d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = 1 # 541 #number of features\n",
    "hidden_size = 50 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 4 #number of output classes \n",
    "# when you have label is 1, it actually means that your label is [0 1 0 0]\n",
    "# if you have label 2 then your label is actually [0 0 1 0]\n",
    "# in some problems you might have label = [0.1 0.2 0.5 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e4c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]) #our lstm class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6cee3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()    # mean-squared error for regression\n",
    "# cross entropy loss is a dual function that implements softmax and cross-entropy\n",
    "# the reason is that, when num_classes is huge (>10000) it is faster to make a dual function instead of calling\n",
    "# nn.LogSoftmax() and torch.nn.NLLLoss() (Negative log-likelihood Loss)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc90ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(214)\n",
      "Epoch: 0, loss: 1.36402, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 100, loss: 1.06741, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 200, loss: 0.99244, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 300, loss: 1.02932, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 400, loss: 1.05342, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 500, loss: 0.99952, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 600, loss: 1.00271, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 700, loss: 1.01195, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 800, loss: 1.02471, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 900, loss: 1.04299, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1000, loss: 1.00767, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1100, loss: 1.00722, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1200, loss: 1.00221, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1300, loss: 1.00778, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1400, loss: 1.06279, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1500, loss: 1.07415, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1600, loss: 1.04084, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1700, loss: 1.07862, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1800, loss: 1.05882, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 1900, loss: 1.06467, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2000, loss: 1.07417, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2100, loss: 1.09857, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2200, loss: 1.07041, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2300, loss: 1.05952, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2400, loss: 1.06117, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2500, loss: 1.05431, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2600, loss: 1.01572, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2700, loss: 0.94829, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2800, loss: 1.01797, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 2900, loss: 1.02535, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3000, loss: 1.05292, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3100, loss: 1.06526, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3200, loss: 1.05646, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3300, loss: 1.02937, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3400, loss: 1.03870, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3500, loss: 1.03841, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3600, loss: 1.02574, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3700, loss: 1.03182, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3800, loss: 1.00829, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 3900, loss: 1.01596, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4000, loss: 1.07664, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4100, loss: 1.08441, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4200, loss: 1.05039, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4300, loss: 1.03717, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4400, loss: 1.01273, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4500, loss: 1.06094, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4600, loss: 1.04429, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4700, loss: 1.10014, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4800, loss: 0.97866, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 4900, loss: 1.01098, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5000, loss: 1.02132, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5100, loss: 1.01464, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5200, loss: 1.06907, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5300, loss: 1.09498, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5400, loss: 1.04186, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5500, loss: 1.06328, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5600, loss: 1.03010, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5700, loss: 1.07137, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5800, loss: 1.01242, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 5900, loss: 1.02916, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 6000, loss: 1.03112, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 6100, loss: 1.09553, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 6200, loss: 1.06829, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 6300, loss: 1.00074, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 6400, loss: 1.00544, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 6500, loss: 1.01498, accuracy: 26.650062\n",
      "tensor(205)\n",
      "Epoch: 6600, loss: 1.01861, accuracy: 25.529265\n",
      "tensor(193)\n",
      "Epoch: 6700, loss: 1.01225, accuracy: 24.034869\n",
      "tensor(198)\n",
      "Epoch: 6800, loss: 0.97399, accuracy: 24.657534\n",
      "tensor(190)\n",
      "Epoch: 6900, loss: 1.05174, accuracy: 23.661270\n",
      "tensor(205)\n",
      "Epoch: 7000, loss: 0.96833, accuracy: 25.529265\n",
      "tensor(198)\n",
      "Epoch: 7100, loss: 1.00545, accuracy: 24.657534\n",
      "tensor(208)\n",
      "Epoch: 7200, loss: 1.05515, accuracy: 25.902864\n",
      "tensor(195)\n",
      "Epoch: 7300, loss: 0.99039, accuracy: 24.283935\n",
      "tensor(202)\n",
      "Epoch: 7400, loss: 1.00974, accuracy: 25.155666\n",
      "tensor(201)\n",
      "Epoch: 7500, loss: 0.96826, accuracy: 25.031133\n",
      "tensor(151)\n",
      "Epoch: 7600, loss: 1.00318, accuracy: 18.804483\n",
      "tensor(204)\n",
      "Epoch: 7700, loss: 1.06329, accuracy: 25.404732\n",
      "tensor(211)\n",
      "Epoch: 7800, loss: 1.00797, accuracy: 26.276463\n",
      "tensor(138)\n",
      "Epoch: 7900, loss: 0.92582, accuracy: 17.185554\n",
      "tensor(196)\n",
      "Epoch: 8000, loss: 0.93725, accuracy: 24.408468\n",
      "tensor(179)\n",
      "Epoch: 8100, loss: 0.97899, accuracy: 22.291407\n",
      "tensor(191)\n",
      "Epoch: 8200, loss: 1.00161, accuracy: 23.785803\n",
      "tensor(193)\n",
      "Epoch: 8300, loss: 0.94540, accuracy: 24.034869\n",
      "tensor(195)\n",
      "Epoch: 8400, loss: 0.96409, accuracy: 24.283935\n",
      "tensor(200)\n",
      "Epoch: 8500, loss: 0.96769, accuracy: 24.906600\n",
      "tensor(189)\n",
      "Epoch: 8600, loss: 0.96529, accuracy: 23.536737\n",
      "tensor(128)\n",
      "Epoch: 8700, loss: 0.95878, accuracy: 15.940224\n",
      "tensor(197)\n",
      "Epoch: 8800, loss: 0.91111, accuracy: 24.533001\n",
      "tensor(194)\n",
      "Epoch: 8900, loss: 1.02899, accuracy: 24.159402\n",
      "tensor(186)\n",
      "Epoch: 9000, loss: 1.00075, accuracy: 23.163138\n",
      "tensor(189)\n",
      "Epoch: 9100, loss: 0.96944, accuracy: 23.536737\n",
      "tensor(170)\n",
      "Epoch: 9200, loss: 0.99424, accuracy: 21.170610\n",
      "tensor(170)\n",
      "Epoch: 9300, loss: 0.96132, accuracy: 21.170610\n",
      "tensor(191)\n",
      "Epoch: 9400, loss: 1.03325, accuracy: 23.785803\n",
      "tensor(167)\n",
      "Epoch: 9500, loss: 0.99239, accuracy: 20.797011\n",
      "tensor(169)\n",
      "Epoch: 9600, loss: 0.91309, accuracy: 21.046077\n",
      "tensor(169)\n",
      "Epoch: 9700, loss: 0.97151, accuracy: 21.046077\n",
      "tensor(167)\n",
      "Epoch: 9800, loss: 0.94151, accuracy: 20.797011\n",
      "tensor(127)\n",
      "Epoch: 9900, loss: 0.97712, accuracy: 15.815691\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'flush'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/master/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/master/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m     \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROTOCOL_VERSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/rp63g11s5fv1rxj158yxndfm0000gn/T/ipykernel_36178/4172379748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# FOR LOADING THE MODEL:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/master/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/master/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_buffer_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'flush'"
     ]
    }
   ],
   "source": [
    "# back-propagation is the algorithm that we use to train neural networks\n",
    "# the core idea of BP is dynamic programming = consuming storage (memory) to store intermediate steps\n",
    "# the large batch size -> the more memory you need -> slow computation\n",
    "# when you are an architect, you build a new network, you need a small batch size to debug\n",
    "# in practice, we always use \"minibatch - learnin\" or \"minibatch stochastic gradient descent\"\n",
    "# instead of using the whole dataset, we give the model a minibatch and update the loss\n",
    "# the idea is that, the statistics of a minibatch can be similar with the dataset\n",
    "# for example: if your data is 1000 dogs 1000 cats and 2000 birds, then by picking randomly 100 samples, you can have \n",
    "# 25 25 50 dogs cats birds -> so the model can still learn\n",
    "\n",
    "import random\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    outputs = model(X_test_tensors_final.float())\n",
    "    outputs = torch.nn.functional.log_softmax(outputs, dim=-1)\n",
    "    predictions = torch.argmax(outputs, 1, keepdim=False)\n",
    "    # predictions = torch.randint(0, 3, y_test.size())\n",
    "    n_correct = torch.eq(predictions, y_test).int().sum()\n",
    "    total = y_test.numel()\n",
    "    \n",
    "    print(n_correct)\n",
    "    accuracy = (n_correct.item() / total) * 100\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "for epoch in range(10000):\n",
    "    iterable_bsz = 128\n",
    "\n",
    "    # we select a random 4 elements from the dataset to train the model\n",
    "    batch_list = list()\n",
    "    #for i in range(iterable_bsz):\n",
    "    #    idx = random.randint(0, X_train_tensors_final.size(0) - 1)\n",
    "    #    batch_list.append(idx)\n",
    "    batch_list = torch.randint(0, X_train_tensors_final.size(0), (iterable_bsz,))\n",
    "    # batch_list = torch.LongTensor(batch_list)\n",
    "    sample = X_train_tensors_final.index_select(0, batch_list)\n",
    "    label = y_train.index_select(0, batch_list)\n",
    "    \n",
    "    outputs = model(sample.float()) #forward pass\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    " \n",
    "  # obtain the loss function\n",
    "    loss = criterion(outputs, label)\n",
    " \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    " \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "    if epoch % 100 == 0:\n",
    "        accuracy = evaluate()\n",
    "        print(\"Epoch: %d, loss: %1.5f, accuracy: %2.6f\" % (epoch, loss.item(), accuracy))\n",
    "        \n",
    "# AFTER TRAINING:\n",
    "\n",
    "# store the weights of model to a dictionary \n",
    "model_weights = model.state_dict()\n",
    "\n",
    "torch.save(\"model.pt\", model_weights)\n",
    "\n",
    "# FOR LOADING THE MODEL:\n",
    "# You need to build the model again\n",
    "#model = LSTM1(.....)\n",
    "\n",
    "weights = torch.load(\"model.pt\")\n",
    "model.load_state_dict(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd60fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e3775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6df115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f11485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
