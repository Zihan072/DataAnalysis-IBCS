{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39455e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from torch.autograd import Variable \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob, os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee739f1",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169606a3",
   "metadata": {},
   "source": [
    "## 1.1 load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d113c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>action mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C27_00</td>\n",
       "      <td>0.027233</td>\n",
       "      <td>0.027539</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>0.027645</td>\n",
       "      <td>0.027516</td>\n",
       "      <td>0.027216</td>\n",
       "      <td>0.026913</td>\n",
       "      <td>0.027065</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035469</td>\n",
       "      <td>0.033350</td>\n",
       "      <td>0.032376</td>\n",
       "      <td>0.031603</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.030428</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C27_01</td>\n",
       "      <td>0.034097</td>\n",
       "      <td>0.033132</td>\n",
       "      <td>0.033106</td>\n",
       "      <td>0.034465</td>\n",
       "      <td>0.033873</td>\n",
       "      <td>0.035325</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.035615</td>\n",
       "      <td>0.033612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>0.033235</td>\n",
       "      <td>0.036673</td>\n",
       "      <td>0.035707</td>\n",
       "      <td>0.032408</td>\n",
       "      <td>0.034262</td>\n",
       "      <td>0.033855</td>\n",
       "      <td>0.035080</td>\n",
       "      <td>0.034525</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C27_02</td>\n",
       "      <td>0.022425</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>0.021829</td>\n",
       "      <td>0.021994</td>\n",
       "      <td>0.021960</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>0.008235</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C27_03</td>\n",
       "      <td>0.116440</td>\n",
       "      <td>0.098673</td>\n",
       "      <td>0.095889</td>\n",
       "      <td>0.085707</td>\n",
       "      <td>0.083466</td>\n",
       "      <td>0.076944</td>\n",
       "      <td>0.073436</td>\n",
       "      <td>0.067148</td>\n",
       "      <td>0.063102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020565</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>0.021047</td>\n",
       "      <td>0.021765</td>\n",
       "      <td>0.021712</td>\n",
       "      <td>0.022141</td>\n",
       "      <td>0.020585</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C27_04</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>0.032558</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>0.034080</td>\n",
       "      <td>0.033181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035469</td>\n",
       "      <td>0.033350</td>\n",
       "      <td>0.032376</td>\n",
       "      <td>0.031603</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.030428</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>GABAA allosteric antagonist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>WT_Control_584</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.012799</td>\n",
       "      <td>0.012655</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>0.012789</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>0.012748</td>\n",
       "      <td>0.012829</td>\n",
       "      <td>0.012829</td>\n",
       "      <td>0.012859</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>WT_Control_585</td>\n",
       "      <td>0.023257</td>\n",
       "      <td>0.022263</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.023273</td>\n",
       "      <td>0.022829</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.022643</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>0.022680</td>\n",
       "      <td>0.023417</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>0.023172</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>WT_Control_586</td>\n",
       "      <td>0.027347</td>\n",
       "      <td>0.026401</td>\n",
       "      <td>0.025863</td>\n",
       "      <td>0.025242</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.025884</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>0.025481</td>\n",
       "      <td>0.025876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023123</td>\n",
       "      <td>0.023913</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>0.024275</td>\n",
       "      <td>0.023605</td>\n",
       "      <td>0.022926</td>\n",
       "      <td>0.022167</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>WT_Control_587</td>\n",
       "      <td>0.024743</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>0.025545</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>0.024599</td>\n",
       "      <td>0.024149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024652</td>\n",
       "      <td>0.028002</td>\n",
       "      <td>0.025262</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>0.026199</td>\n",
       "      <td>0.025387</td>\n",
       "      <td>0.024102</td>\n",
       "      <td>0.028028</td>\n",
       "      <td>0.025369</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>WT_Control_588</td>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.029050</td>\n",
       "      <td>0.028944</td>\n",
       "      <td>0.038098</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.026695</td>\n",
       "      <td>0.027058</td>\n",
       "      <td>0.028158</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027214</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.025715</td>\n",
       "      <td>0.025756</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.026659</td>\n",
       "      <td>0.027757</td>\n",
       "      <td>WT_control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4803 rows × 543 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               index         0         1         2         3         4  \\\n",
       "0             C27_00  0.027233  0.027539  0.027299  0.027645  0.027516   \n",
       "1             C27_01  0.034097  0.033132  0.033106  0.034465  0.033873   \n",
       "2             C27_02  0.022425  0.022276  0.022578  0.021829  0.021994   \n",
       "3             C27_03  0.116440  0.098673  0.095889  0.085707  0.083466   \n",
       "4             C27_04  0.034495  0.034277  0.032969  0.033982  0.032666   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "4798  WT_Control_584  0.012645  0.012774  0.012832  0.012799  0.012655   \n",
       "4799  WT_Control_585  0.023257  0.022263  0.022948  0.021674  0.023818   \n",
       "4800  WT_Control_586  0.027347  0.026401  0.025863  0.025242  0.025504   \n",
       "4801  WT_Control_587  0.024743  0.024200  0.024444  0.023782  0.025545   \n",
       "4802  WT_Control_588  0.028297  0.029050  0.028944  0.038098  0.036434   \n",
       "\n",
       "             5         6         7         8  ...       532       533  \\\n",
       "0     0.027216  0.026913  0.027065  0.027211  ...  0.035469  0.033350   \n",
       "1     0.035325  0.035155  0.035615  0.033612  ...  0.033993  0.033235   \n",
       "2     0.021960  0.021944  0.022157  0.022356  ...  0.007974  0.008208   \n",
       "3     0.076944  0.073436  0.067148  0.063102  ...  0.020565  0.020161   \n",
       "4     0.032558  0.032460  0.034080  0.033181  ...  0.035469  0.033350   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "4798  0.012602  0.012633  0.012648  0.012643  ...  0.012822  0.012893   \n",
       "4799  0.023273  0.022829  0.021736  0.022251  ...  0.023851  0.022643   \n",
       "4800  0.025884  0.025644  0.025481  0.025876  ...  0.023123  0.023913   \n",
       "4801  0.025843  0.025644  0.024599  0.024149  ...  0.024652  0.028002   \n",
       "4802  0.026695  0.027058  0.028158  0.027984  ...  0.027214  0.027715   \n",
       "\n",
       "           534       535       536       537       538       539       540  \\\n",
       "0     0.032376  0.031603  0.030481  0.030428  0.033246  0.033398  0.035958   \n",
       "1     0.036673  0.035707  0.032408  0.034262  0.033855  0.035080  0.034525   \n",
       "2     0.008466  0.008117  0.007963  0.008235  0.008228  0.008159  0.008054   \n",
       "3     0.021047  0.021765  0.021712  0.022141  0.020585  0.020312  0.020073   \n",
       "4     0.032376  0.031603  0.030481  0.030428  0.033246  0.033398  0.035958   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4798  0.012789  0.012867  0.012767  0.012748  0.012829  0.012829  0.012859   \n",
       "4799  0.023834  0.022680  0.023417  0.023315  0.023172  0.022639  0.021779   \n",
       "4800  0.025340  0.025306  0.023730  0.024275  0.023605  0.022926  0.022167   \n",
       "4801  0.025262  0.028470  0.026199  0.025387  0.024102  0.028028  0.025369   \n",
       "4802  0.027027  0.025767  0.025715  0.025756  0.027127  0.026659  0.027757   \n",
       "\n",
       "                      action mode  \n",
       "0     GABAA allosteric antagonist  \n",
       "1     GABAA allosteric antagonist  \n",
       "2     GABAA allosteric antagonist  \n",
       "3     GABAA allosteric antagonist  \n",
       "4     GABAA allosteric antagonist  \n",
       "...                           ...  \n",
       "4798                   WT_control  \n",
       "4799                   WT_control  \n",
       "4800                   WT_control  \n",
       "4801                   WT_control  \n",
       "4802                   WT_control  \n",
       "\n",
       "[4803 rows x 543 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./C5-C129/data_frames_labeled/effected_all_frames_labeled.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71456cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027233</td>\n",
       "      <td>0.027539</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>0.027645</td>\n",
       "      <td>0.027516</td>\n",
       "      <td>0.027216</td>\n",
       "      <td>0.026913</td>\n",
       "      <td>0.027065</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.027248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>0.035469</td>\n",
       "      <td>0.033350</td>\n",
       "      <td>0.032376</td>\n",
       "      <td>0.031603</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.030428</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.035958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034097</td>\n",
       "      <td>0.033132</td>\n",
       "      <td>0.033106</td>\n",
       "      <td>0.034465</td>\n",
       "      <td>0.033873</td>\n",
       "      <td>0.035325</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.035615</td>\n",
       "      <td>0.033612</td>\n",
       "      <td>0.034037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>0.033235</td>\n",
       "      <td>0.036673</td>\n",
       "      <td>0.035707</td>\n",
       "      <td>0.032408</td>\n",
       "      <td>0.034262</td>\n",
       "      <td>0.033855</td>\n",
       "      <td>0.035080</td>\n",
       "      <td>0.034525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022425</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>0.021829</td>\n",
       "      <td>0.021994</td>\n",
       "      <td>0.021960</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008156</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>0.008235</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.008054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.116440</td>\n",
       "      <td>0.098673</td>\n",
       "      <td>0.095889</td>\n",
       "      <td>0.085707</td>\n",
       "      <td>0.083466</td>\n",
       "      <td>0.076944</td>\n",
       "      <td>0.073436</td>\n",
       "      <td>0.067148</td>\n",
       "      <td>0.063102</td>\n",
       "      <td>0.059446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.020565</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>0.021047</td>\n",
       "      <td>0.021765</td>\n",
       "      <td>0.021712</td>\n",
       "      <td>0.022141</td>\n",
       "      <td>0.020585</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>0.020073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034495</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>0.032558</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>0.034080</td>\n",
       "      <td>0.033181</td>\n",
       "      <td>0.034841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>0.035469</td>\n",
       "      <td>0.033350</td>\n",
       "      <td>0.032376</td>\n",
       "      <td>0.031603</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.030428</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.035958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>0.012645</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.012799</td>\n",
       "      <td>0.012655</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>0.012804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>0.012789</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>0.012748</td>\n",
       "      <td>0.012829</td>\n",
       "      <td>0.012829</td>\n",
       "      <td>0.012859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>0.023257</td>\n",
       "      <td>0.022263</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.023273</td>\n",
       "      <td>0.022829</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>0.023746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023855</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.022643</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>0.022680</td>\n",
       "      <td>0.023417</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>0.023172</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.021779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>0.027347</td>\n",
       "      <td>0.026401</td>\n",
       "      <td>0.025863</td>\n",
       "      <td>0.025242</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.025884</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>0.025481</td>\n",
       "      <td>0.025876</td>\n",
       "      <td>0.025694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023655</td>\n",
       "      <td>0.023123</td>\n",
       "      <td>0.023913</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>0.024275</td>\n",
       "      <td>0.023605</td>\n",
       "      <td>0.022926</td>\n",
       "      <td>0.022167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>0.024743</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>0.025545</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>0.024599</td>\n",
       "      <td>0.024149</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.024652</td>\n",
       "      <td>0.028002</td>\n",
       "      <td>0.025262</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>0.026199</td>\n",
       "      <td>0.025387</td>\n",
       "      <td>0.024102</td>\n",
       "      <td>0.028028</td>\n",
       "      <td>0.025369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>0.028297</td>\n",
       "      <td>0.029050</td>\n",
       "      <td>0.028944</td>\n",
       "      <td>0.038098</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.026695</td>\n",
       "      <td>0.027058</td>\n",
       "      <td>0.028158</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>0.028212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027264</td>\n",
       "      <td>0.027214</td>\n",
       "      <td>0.027715</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.025715</td>\n",
       "      <td>0.025756</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.026659</td>\n",
       "      <td>0.027757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4803 rows × 541 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.027233  0.027539  0.027299  0.027645  0.027516  0.027216  0.026913   \n",
       "1     0.034097  0.033132  0.033106  0.034465  0.033873  0.035325  0.035155   \n",
       "2     0.022425  0.022276  0.022578  0.021829  0.021994  0.021960  0.021944   \n",
       "3     0.116440  0.098673  0.095889  0.085707  0.083466  0.076944  0.073436   \n",
       "4     0.034495  0.034277  0.032969  0.033982  0.032666  0.032558  0.032460   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4798  0.012645  0.012774  0.012832  0.012799  0.012655  0.012602  0.012633   \n",
       "4799  0.023257  0.022263  0.022948  0.021674  0.023818  0.023273  0.022829   \n",
       "4800  0.027347  0.026401  0.025863  0.025242  0.025504  0.025884  0.025644   \n",
       "4801  0.024743  0.024200  0.024444  0.023782  0.025545  0.025843  0.025644   \n",
       "4802  0.028297  0.029050  0.028944  0.038098  0.036434  0.026695  0.027058   \n",
       "\n",
       "             7         8         9  ...       531       532       533  \\\n",
       "0     0.027065  0.027211  0.027248  ...  0.035112  0.035469  0.033350   \n",
       "1     0.035615  0.033612  0.034037  ...  0.034600  0.033993  0.033235   \n",
       "2     0.022157  0.022356  0.021943  ...  0.008156  0.007974  0.008208   \n",
       "3     0.067148  0.063102  0.059446  ...  0.021132  0.020565  0.020161   \n",
       "4     0.034080  0.033181  0.034841  ...  0.035112  0.035469  0.033350   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4798  0.012648  0.012643  0.012804  ...  0.012812  0.012822  0.012893   \n",
       "4799  0.021736  0.022251  0.023746  ...  0.023855  0.023851  0.022643   \n",
       "4800  0.025481  0.025876  0.025694  ...  0.023655  0.023123  0.023913   \n",
       "4801  0.024599  0.024149  0.024176  ...  0.024527  0.024652  0.028002   \n",
       "4802  0.028158  0.027984  0.028212  ...  0.027264  0.027214  0.027715   \n",
       "\n",
       "           534       535       536       537       538       539       540  \n",
       "0     0.032376  0.031603  0.030481  0.030428  0.033246  0.033398  0.035958  \n",
       "1     0.036673  0.035707  0.032408  0.034262  0.033855  0.035080  0.034525  \n",
       "2     0.008466  0.008117  0.007963  0.008235  0.008228  0.008159  0.008054  \n",
       "3     0.021047  0.021765  0.021712  0.022141  0.020585  0.020312  0.020073  \n",
       "4     0.032376  0.031603  0.030481  0.030428  0.033246  0.033398  0.035958  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4798  0.012789  0.012867  0.012767  0.012748  0.012829  0.012829  0.012859  \n",
       "4799  0.023834  0.022680  0.023417  0.023315  0.023172  0.022639  0.021779  \n",
       "4800  0.025340  0.025306  0.023730  0.024275  0.023605  0.022926  0.022167  \n",
       "4801  0.025262  0.028470  0.026199  0.025387  0.024102  0.028028  0.025369  \n",
       "4802  0.027027  0.025767  0.025715  0.025756  0.027127  0.026659  0.027757  \n",
       "\n",
       "[4803 rows x 541 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:, 1:-1]\n",
    "#data_array = \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcaea1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='action mode', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAJNCAYAAACBe1nxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkd0lEQVR4nO3de5hkdX3n8c9XxhteQdAoiEMMXvCGYSReopJoFLObYCIqRIVoEtT1mmc10TzZaHSJGJMYL/FCjAFcRfGCQtQIISoRURwUZgBvKEQRAqO4isbggr/94/xaiqZn6MHp6d/0vF7P00+fOnVO1a+rT1W/69SprmqtBQCA8dxkuQcAAMDChBoAwKCEGgDAoIQaAMCghBoAwKBWLfcAlsouu+zSVq9evdzDAAC4QWeddda3W2u7zp+/YkNt9erVWbt27XIPAwDgBlXVvy8030ufAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAg1q13AMA2B487A0PW+4hcCOd/rzTl3sIbMfsUQMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGNSShVpV3bWqPl5VX6yq86rqBX3+zlV1SlV9tX/faWadl1bVBVX15ap67Mz8fatqfT/v9VVVSzVuAIBRLOUetauT/M/W2r2TPDjJc6pq7yQvSXJqa22vJKf20+nnHZzkPkkOSPKmqtqhX9abkxyeZK/+dcASjhsAYAhLFmqttUtba5/v01cm+WKS3ZIcmOSYvtgxSR7fpw9M8u7W2lWttQuTXJBkv6q6c5LbttbOaK21JMfOrAMAsGJtlWPUqmp1kgcm+WySO7XWLk2mmEtyx77Ybkm+ObPaxX3ebn16/vyFrufwqlpbVWs3bNiwRX8GAICtbclDrapuneT9SV7YWvv+phZdYF7bxPzrz2ztqNbamtbaml133XXzBwsAMJAlDbWqummmSHtna+0DffZl/eXM9O+X9/kXJ7nrzOq7J7mkz999gfkAACvaUr7rs5L8Q5Ivttb+ZuasE5Mc1qcPS/KhmfkHV9XNq2rPTG8aOLO/PHplVT24X+ahM+sAAKxYq5bwsh+W5GlJ1lfV2X3enyQ5MsnxVfV7Sb6R5IlJ0lo7r6qOT3J+pneMPqe1dk1f79lJjk5yyyQf7V8AACvakoVaa+1TWfj4siR51EbWOSLJEQvMX5vkvltudAAA4/PJBAAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAg1qyUKuqt1fV5VV17sy8l1fVt6rq7P716zPnvbSqLqiqL1fVY2fm71tV6/t5r6+qWqoxAwCMZCn3qB2d5IAF5r+2tbZP//pIklTV3kkOTnKfvs6bqmqHvvybkxyeZK/+tdBlAgCsOEsWaq2105JcscjFD0zy7tbaVa21C5NckGS/qrpzktu21s5orbUkxyZ5/JIMGABgMMtxjNpzq2pdf2l0pz5vtyTfnFnm4j5vtz49f/6CqurwqlpbVWs3bNiwpccNALBVbe1Qe3OSuyfZJ8mlSf66z1/ouLO2ifkLaq0d1Vpb01pbs+uuu/6MQwUAWF5bNdRaa5e11q5prf0kyd8n2a+fdXGSu84sunuSS/r83ReYDwCw4m3VUOvHnM35rSRz7wg9McnBVXXzqtoz05sGzmytXZrkyqp6cH+356FJPrQ1xwwAsFxWLdUFV9VxSfZPsktVXZzkZUn2r6p9Mr18eVGSZyZJa+28qjo+yflJrk7ynNbaNf2inp3pHaS3TPLR/gUAsOItWai11g5ZYPY/bGL5I5IcscD8tUnuuwWHBgCwTfDJBAAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAgxJqAACDEmoAAIMSagAAg1pUqFXVqYuZBwDAlrNqU2dW1S2S7Jhkl6raKUn1s26b5C5LPDYAgO3aJkMtyTOTvDBTlJ2Va0Pt+0n+bumGBQDAJkOttfa6JK+rque11t6wlcYEAEBueI9akqS19oaqemiS1bPrtNaOXaJxAQBs9xYValX1jiR3T3J2kmv67JZEqAEALJFFhVqSNUn2bq21pRwMAADXWuz/UTs3yc8t5UAAALiuxe5R2yXJ+VV1ZpKr5ma21n5zSUYFAMCiQ+3lSzkIAACub7Hv+vzkUg8EAIDrWuy7Pq/M9C7PJLlZkpsm+WFr7bZLNTAAgO3dYveo3Wb2dFU9Psl+SzEgAAAmiz1G7Tpaax+sqpds6cHAtuQbr7jfcg+Bn8Eef7Z+uYcAcIMW+9Lnb8+cvEmm/6vmf6oBACyhxe5R+42Z6auTXJTkwC0+GgAAfmqxx6g9fakHAgDAdS3qkwmqaveqOqGqLq+qy6rq/VW1+1IPDgBge7bYj5D6xyQnJrlLkt2SnNTnAQCwRBYbaru21v6xtXZ1/zo6ya5LOC4AgO3eYkPt21X11KraoX89Ncl3lnJgAADbu8WG2jOSPCnJfyS5NMlBSbzBAABgCS3233O8MslhrbXvJklV7ZzkrzIFHAAAS2Cxe9TuPxdpSdJauyLJA5dmSAAAJIsPtZtU1U5zJ/oetRv18VMAACzOYmPrr5N8uqrel+mjo56U5IglGxUAAIv+ZIJjq2ptkl9NUkl+u7V2/pKODABgO7foly97mIkzAICtZLHHqAEAsJUJNQCAQQk1AIBBCTUAgEEJNQCAQQk1AIBBCTUAgEEJNQCAQQk1AIBBCTUAgEEJNQCAQQk1AIBBCTUAgEEJNQCAQQk1AIBBCTUAgEEtWahV1dur6vKqOndm3s5VdUpVfbV/32nmvJdW1QVV9eWqeuzM/H2ran0/7/VVVUs1ZgCAkSzlHrWjkxwwb95LkpzaWtsryan9dKpq7yQHJ7lPX+dNVbVDX+fNSQ5Pslf/mn+ZAAAr0pKFWmvttCRXzJt9YJJj+vQxSR4/M//drbWrWmsXJrkgyX5Vdeckt22tndFaa0mOnVkHAGBF29rHqN2ptXZpkvTvd+zzd0vyzZnlLu7zduvT8+cDAKx4o7yZYKHjztom5i98IVWHV9Xaqlq7YcOGLTY4AIDlsLVD7bL+cmb698v7/IuT3HVmud2TXNLn777A/AW11o5qra1pra3Zddddt+jAAQC2tq0daicmOaxPH5bkQzPzD66qm1fVnpneNHBmf3n0yqp6cH+356Ez6wAArGirluqCq+q4JPsn2aWqLk7ysiRHJjm+qn4vyTeSPDFJWmvnVdXxSc5PcnWS57TWrukX9exM7yC9ZZKP9i8AgBVvyUKttXbIRs561EaWPyLJEQvMX5vkvltwaAAA24RR3kwAAMA8Qg0AYFBCDQBgUEINAGBQQg0AYFBCDQBgUEINAGBQQg0AYFBCDQBgUEINAGBQQg0AYFBCDQBgUEINAGBQQg0AYFBCDQBgUEINAGBQQg0AYFBCDQBgUEINAGBQQg0AYFBCDQBgUEINAGBQQg0AYFBCDQBgUEINAGBQQg0AYFCrlnsAo9n3xccu9xC4kc56zaHLPQQA2KLsUQMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGNSq5R4AAHCtTz7ikcs9BH4Gjzztk1v08uxRAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABiUUAMAGJRQAwAYlFADABjUsoRaVV1UVeur6uyqWtvn7VxVp1TVV/v3nWaWf2lVXVBVX66qxy7HmAEAtrbl3KP2K621fVpra/rplyQ5tbW2V5JT++lU1d5JDk5ynyQHJHlTVe2wHAMGANiaRnrp88Akx/TpY5I8fmb+u1trV7XWLkxyQZL9tv7wAAC2ruUKtZbk5Ko6q6oO7/Pu1Fq7NEn69zv2+bsl+ebMuhf3eddTVYdX1dqqWrthw4YlGjoAwNaxapmu92GttUuq6o5JTqmqL21i2VpgXltowdbaUUmOSpI1a9YsuAwAwLZiWfaotdYu6d8vT3JCppcyL6uqOydJ/355X/ziJHedWX33JJdsvdECACyPrR5qVXWrqrrN3HSSxyQ5N8mJSQ7rix2W5EN9+sQkB1fVzatqzyR7JTlz644aAGDrW46XPu+U5ISqmrv+d7XW/rmqPpfk+Kr6vSTfSPLEJGmtnVdVxyc5P8nVSZ7TWrtmGcYNALBVbfVQa619PckDFpj/nSSP2sg6RyQ5YomHBgAwlJH+PQcAADOEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKCEGgDAoIQaAMCghBoAwKC2mVCrqgOq6stVdUFVvWS5xwMAsNS2iVCrqh2S/F2SxyXZO8khVbX38o4KAGBpbROhlmS/JBe01r7eWvtxkncnOXCZxwQAsKSqtbbcY7hBVXVQkgNaa7/fTz8tyS+11p47b7nDkxzeT94zyZe36kDHt0uSby/3INhm2F5YLNsKm8P2srC7tdZ2nT9z1XKM5EaoBeZdrzBba0clOWrph7Ntqqq1rbU1yz0Otg22FxbLtsLmsL1snm3lpc+Lk9x15vTuSS5ZprEAAGwV20qofS7JXlW1Z1XdLMnBSU5c5jEBACypbeKlz9ba1VX13CQfS7JDkre31s5b5mFti7wszOawvbBYthU2h+1lM2wTbyYAANgebSsvfQIAbHeEGgDAoFZsqFXVnarqXVX19ao6q6rOqKrfmrfM66rqW1V1k5l5v1tVG6rq7Ko6r6reV1U7zlvvnKo6biPX+6GqOuNGjHf/qvqnmTG88UZcxj5V9es3Yr27VNX7Nne9zbj81VX1O0t1+Ru5zt/c1EeN3djbajOuf5va/rak/vs+dyPnfaKqNvtt+VX18qp60c8+upWnqu7Qt5ezq+o/+jY1d7r17+dW1UlVdfu+zuqq+lE/7/yqektV3aSqLqyqe867/L+tqj9ahp9rTVW9fhPnb/XHFVgOKzLUqqqSfDDJaa21n2+t7ZvpnaK7zyxzkyS/leSbSR4x7yLe01rbp7V2nyQ/TvLkmfXunel2e0RV3Wre9d4+yS8muX1V7bmlf65F2CfJZsVHVa1qrV3SWjtoaYaUJFmdZKs+oLbWTmytHbmJRfbJZt5Wi7VSt7/+UW7bnG113IvVWvtO3172SfKWJK+dOf3DPn3fJFckec7Mql/ry9w/00fzPT7Tp74cPLdA304PSvKerfCjXEdrbW1r7fmbWGR1tvLjykpWVa+tqhfOnP5YVb1t5vT7q+r7Pe6v6FF/dlX9yxKP68bugFgxT+5WZKgl+dUkP26tvWVuRmvt31trb5hZ5leSnJvkzUkOWehCqmpVklsl+e7M7N9J8o4kJyf5zXmrPCHJSZn3YDfvMverqk9X1Rf693sutNzM8nerqlOral3/vkef/8T+LPmcqjqtpn9b8ookT+53nidX1a2q6u1V9bl+fQf2dX+3qt5bVSclOXl2D0hV7VBVf1VV6/t1Pm+BMf1Bv8xz+p13xz7/6Kp6ff+5vl7TJ0okyZFJHt7H9Yf9+v6tqj7fvx7a179JVb2ppj1J/1RVH5m7jKp6VP8Z1vef6eZ9/kVV9ef9ctZX1b1mfsY3Lva22tTv4EYYeft7eVW9o6r+taq+WlV/0OdXVb2m307r526Tmvb0fryq3pVkfd8+XtN//+uq6pkbuQ1WVdUxfZnr7RXsl31Iv65zq+rVM/MP6L/Pc6rq1AXW+4Oq+mhV3bKqnlpVZ/bf41urR1lV/aCqXlFVn03ykI2McXtzRpLd5s9srV2d5NNJfiHJcbnutvOIJBe11v59dp2qunV/PJq73x04c97/qqovVdUpVXVc9T+WNf3B/UzfJk6oqp36/E9U1av77/ErVfXwPn/2VYZH1rV7Cb9QVbfJvMeVLXg7ba8+neSnj8WZPj3gPjPn3yXJr/W4PzHJi/uTgEcv8bj2yUaeVPfHyJWvtbbivpI8P9Ozyk0t87YkT0ty2yTfSnLTPv93k2xIcnaSy5L8W5IdZtb7SpK7JXlMkhPnXea/JHl4knskWbeR671tklV9+tFJ3t+n90/yTzNjeGOfPinJYX36GUk+2KfXJ9mtT99+/nr99F8keercMn3st+rLXZxk537e6iTn9ulnJ3n/zBh3XuBnuMPM9P9O8rw+fXSS92Z6ArB3ps9nvc7P1k/vmOQWfXqvJGv79EFJPtLX/7lMgXJQkltk2vN0j77csUle2Kcvmrn+/5HkbQvchjd4W21H29/Lk5yT5JaZHoi/mekB+AlJTsn072/ulOQbSe7cf3c/TLJnX//wJH/ap2+eZO3ceTPXsTrTJ4c8rJ9+e5IX9elPJFnTr/MbSXbN9G+C/jXTHp1d+5jmrm/nmXG/KMlzM/2RuHmSe2e6f8zddm9KcmifbkmetNSPNaN9zd1OM6d/0L/vkOm+ecDM72juPr9jpv9V+bh++rwkD+jTb0nynAWuZ1WS2/bpXZJckOkTZNb0bfeWSW6T5Kszv/t1SR7Zp1+R5G9ntom/7tO/nuRf+vT+ufYx8aSZ7enW/fp/er6vLbLt3CXJxX36fkmOyfSEcKd+f/u/SW7Wzz86yUGLuMw/yvT4e06SI/u8fZJ8pm8PJyTZaWY7eHWSMzM9zj08yc3648TcY+KT+zZ+VB/buzI9Hp7aL+/UJHssdF/Ylr9W6h6166iqv+vPzj/XT98s0wPCB1tr30/y2Ux/+Oa8p03PGn4u00b24r7eg5JsaNOzy1OT/OLMs8I7ZXpG+qnW2leSXF1V911gOLdL8t6a9mC9Ntd9xrKQh2TaGJNpT8ov9+nTkxzd94hs7KWdxyR5SVWdnelOcIske/TzTmmtXbHAOo9O8pY2PcvORpa5b98jtj7JU+b9DB9srf2ktXZ+pj/4C7lpkr/v6783U9Sl/2zv7ev/R5KP9/n3THJhv12T6QFk9uXCD/TvZ2X6AzTfYm6rJTPY9pckH2qt/ai19u1Mt/F+mW7741pr17TWLkvyySQP6suf2Vq7sE8/JsmhfZv6bJI7ZIrt+b7ZWju9T/+fXLvdznlQkk+01jb0be2dmX6nD870kvGFyfW2v6cleVySJ7TWrkryqCT7JvlcH8+jkvx8X/aaTE84tne37LfNd5LsnCnG59y9n3d6kg+31j7a5x+X5OC+t+LATPfR+SrJX1TVukxPEHbLdH//5Vy7fV2ZKbBSVbfL9CTpk339G3Mf/puqen6/nKsX9+OzWK21SzI9buyRac/aGZnu4w/JFODrWms/XuzlVdXjMj35+qXW2gOS/GU/69gkf9xau3+mx7eXzay2qrW2X5IXJnlZv74/y7WHg8y9BL9vkgNba7+T5I1Jju2X984kGz2ucVu1UkPtvEzH6iRJWmvPyfQgPvdhpwdkCqb1VXVRpgeX67381KYsPynXPqAckuRefZ2vZdob8oR+3pMzPfO4sJ+/Ogu//PTKJB9v0zEjv5EpnjZH62N7VpI/zfTRWmdX1R0WWLYy/VHbp3/t0Vr7Yj/vhxu5/MoCn6M6z9FJnttau1+SP5/3M1w177IW8oeZ9hY9INMDwM1uYPmNzZ9/nddkgX/ivMjbaksaeftLrv/7bdn0bTy7rVSmPZhz29SerbWTF3kdszb1u97Y9ndupp9r95llj5kZyz1bay/v5/1Xa+2ajVzO9uRHPfrvlul+dr1j1FprD5y53ZIp1J6U6Unbutba5Qtc7lMybc/79su/LNPjwA3dVzfmhu7DRyb5/Ux76j5T/RAHtrjTM0XaXKidMXP605t5WY9O8o+ttf9MpiddWyDY55zYWvtRn97YzowVY6WG2r8muUVVPXtm3uwxMock+f3W2urW2uokeyZ5zELH0WT6pX+tv2b/xCT3n1nvwFz7B/aQTC8rzJ03dwD5fLfL9FJXMr3MdUM+PXM5T0nyqSSpqru31j7bWvuzJN/OFCFXZnq5Yc7Hkjyvqqqv88BFXN/JSZ4199p/Ve28wDK3SXJpVd20j+mGzB/X7ZJc2lr7Saa9JHN7uT6V5Ak1Hat2p0wvbSTJl5Ksrqpf6KeflmmPz6Is8rbakkbe/pLkwKq6RQ/W/TO97HVapmP2dqiqXTM9eJ65wLofS/Ls/rtPVd2j5r2podujquaODTskfbud8dkkj6yqXfpxZYdk+p2e0efv2S9/dvv7QpJnJjmxqu6Saa/iQVV1x7llq+puG/mZt2utte9lekn+RXO/u00s+7VMe+COzBRtC7ldkstba/+vqn4lUwgm0+/5N/r2desk/23m+r87d/xZbtx9eH1r7dWZXm6/V5b2Pry9mjtO7X6Znhh9JlMIPTRTxG2OxTzpn2+TwT5jYzsaciOuc3grMtT6nojHZ3rAv7CqzsxU7n/c/xg+NsmHZ5b/YfoDTJ81d5D5uiQPzLQX7BFJvtVa+9a115TTkuzd/yDtkWmjnrvMC5N8v6p+ad7w/jLJq6rq9CzuZbjnJ3l6H8vTkrygz39NP4j33D6OczK9jLV3XXuA/Cszvcy4ri/3ykVc39syHROwrqrOycLvqvpfmf7QnpIpom7Iuky71M/pB/2+KclhVfWZTMdTzd3p3p/p2Llzk7y1X8f3Wmv/leTpmV4yXp/kJ5mOnVmsxdxWW8zg218yBdiH+/Kv7C95nJDp93ROptD8o/7y83xvS3J+ks/32/OtWfgB9YuZfsfrMr3k9uZ5t9GlSV6a6fdwTpLPt9Y+1FrbkOk4uA/07e8989b7VKZj1T6c5PJMe0pP7tdzSqbj6lhAa+0LmW7rjQX8rOMyxdAJGzn/nUnWVNXaTE/WvtSv43OZjiE8J9PekbVJvtfXOSzTfXFdpuOUXrEZw39h9TcEJflRko/m+o8r/OxOT/Lfk1zRD4O4ItPxzQ/J9CRqc5yc5Bl17ZvNdr6RwX5DQb7gzoyVxEdIMZSqunVr7Qd9b8+ZmQ4gXigYuBGq6uWZDjD/q+UeCyvTzH14x0xPJg5vrX1+ucfFDet7t7+b5PWttT/t845O8pDW2j1nljs60xs5Nvn/N2v6X5aHZvo3Qx9prf1JVe2T6Yn2jkm+nuTprbXvVtUnMh38v7aqdsn0JrPVfa/6xzLtdHhVpjcR/fQxrKpWZ3rD0i6Z3nTw9NbaN1bSY51QYyj9znr7TMfT/GVr7ejlHM9Ks5IevBhTTf/KZe9Mx6wd01p71TIPCbZpQg0AYFDbxz+LAwC2uKq6X6Z3W866qrW20PGx3Aj2qAEADGpFvusTAGAlEGoAAIMSasCKV9MHfD905vSzqurQ5RzTQqrqB8s9BmAs3kwAbA/2T/KD9I/Baa1tzj9MBlg29qgB26Sq+mBVnVVV51XV4TPzD6iqz/f/WH9q/4eYz0ryh/0THx5eVS+vqhf15fepqs9U1bqqOqGu/aD7T1TVq6vqzKr6ysx/U58dw/5V9cmqOr4vc2RVPaWvs76q7t6Xu1sfy7r+fY8+f8+qOqOqPldVr5x32S/u89dV1Z8v2Q0JDE2oAduqZ7TW9k2yJsnzq+oONX1O6d8neUJr7QFJnthauyjTf0J/bf8Q8n+bdznHJvnj1tr9k6xP8rKZ81a11vZL8sJ582c9INNHu90v00fi3KOv87Ykz+vLvDHJsf063pnk9X3+65K8ubX2oCQ//QSOqnpMkr2S7Jfp45b2rarZD68GthNCDdhWPb9/9uNnktw1U9g8OMlp/bNO0z+rcKOq6nZJbt9am/u8wWMyfa7qnA/072clWb2Ri/lca+3S1tpVSb6W6TMOkyn65tZ5SJJ39el3JPnlPv2wXPvB57P/i+ox/esLST6f6XM399rUzwKsTI5RA7Y5VbV/kkdn+gzC/+wfPXaLJJVkS/5zyKv692uy8cfLq2amfzJz+iebWKdtZHpOJXlVa+2tixwnsELZowZsi26X5Ls90u6VaU9akpyR5JFVtWeS9A90TpIrk9xm/oW01r6X5Lszx589Lckn5y+3BXw6ycF9+ilJPtWnT583f87Hkjyjqm6dJFW1W1XdcQnGBQzOHjVgW/TPSZ5VVeuSfDnTy59prW3obyz4QFXdJMnlSX4tyUlJ3ldVB+ba48bmHJbkLVW1Y5KvJ3n6Eoz3+UneXlUvTrJh5jpekORdVfWCJO+fW7i1dnJV3TvJGVWVTO9YfWr/eYDtiI+QAgAYlJc+AQAGJdQAAAYl1AAABiXUAAAGJdQAAAYl1AAABiXUAAAG9f8BYgP7NpW8qFAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.countplot(x='action mode', data=data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7722beae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(data['action mode'])\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62568291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GABAA allosteric antagonist', 'GABAA pore blocker',\n",
       "       'TRPV agonist', 'WT_control'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2idx = {\n",
    "    'GABAA allosteric antagonist':0,\n",
    "    'GABAA pore blocker':1,\n",
    "    'TRPV agonist':2,\n",
    "    'WT_control':3,\n",
    "}\n",
    "\n",
    "idx2class = {v: k for k, v in class2idx.items()}\n",
    "\n",
    "data['action mode'].replace(class2idx, inplace=True)\n",
    "\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc13ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.array(X))\n",
    "Y = torch.tensor(np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ecfe1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4803, 541])\n",
      "torch.Size([4803])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6baecac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 541])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:4000, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c79445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = Y.reshape(-1, 1)\n",
    "# Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964f36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 4000 for training\n",
    "\n",
    "X_train = X[:4000, :]\n",
    "X_test = X[4000:, :]\n",
    "\n",
    "y_train = Y[:4000]\n",
    "y_test = Y[4000:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e7534d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape torch.Size([4000, 541]) torch.Size([4000])\n",
      "Testing Shape torch.Size([803, 541]) torch.Size([803])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Shape\", X_train.shape, y_train.shape)\n",
    "print(\"Testing Shape\", X_test.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89c6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping to rows, timestamps, features\n",
    "\n",
    "X_train_tensors_final = torch.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "\n",
    "X_test_tensors_final = torch.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c63aa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape torch.Size([4000, 541, 1]) torch.Size([4000])\n",
      "Testing Shape torch.Size([803, 541, 1]) torch.Size([803])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensors_final = X_train_tensors_final.transpose(1, 2)\n",
    "X_test_tensors_final = X_test_tensors_final.transpose(1, 2)\n",
    "\n",
    "# In computer memory: there is no tensor / 2d Matrix or anything, there is only array\n",
    "# In order to let the computer understand that this is a matrix, we need to tell the computer how to access the elements in the array.\n",
    "# For example: a matrix size (20, 50) is array that has 1000 elements but we tell the computer that it has 20 rows.\n",
    "\n",
    "# So tranpose doesn't change the order of the elements in the array, but only the information of row and collumn\n",
    "# Reshape in some cases can change the order of the elements in the array\n",
    "# The computation can only be done when the order of the array matches the information of the row/collumns\n",
    "# So Reshape is sometimes necessary to avoid errors\n",
    "print(\"Training Shape\", X_train_tensors_final.shape, y_train.shape)\n",
    "print(\"Testing Shape\", X_test_tensors_final.shape, y_test.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aea37d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the batch has 4000 sequences, each sequence has 541 timesteps, each timestep has 1 feature\n",
    "# the \"label batch\"\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, hidden_size) #fully connected 1\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward function of the model.\n",
    "    We should imagine the size of the tensors in the forward function\n",
    "    * batch_size: the number of samples to be computed at the same time\n",
    "    when we have multiple samples, the loss is the sum or average of the loss for each sample.\n",
    "    batch_size is generally the same throughout one forward pass.\n",
    "    for example: batch_size of the input is 1000 then batch size  of the output is also 1000.\n",
    "    -> batch_size is rarely changed during one training step.\n",
    "    \n",
    "    * time: how many steps are there for each sample in the data, how long the data is\n",
    "    - note 1: in your dataset all samples have the same length\n",
    "    - note 2: in some cases the samples have different lengths > more difficult \n",
    "    \n",
    "    * hidden_size: how many features the data has for every time step\n",
    "    \"\"\"\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        inputs: x: torch.Tensor size: [bsz * time * input_hidden_size]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        # the initial states of the recurrent net: each layer has 1 state h_0 and 1 state c_0\n",
    "        # each state h_0 has bsz * hidden_size\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device) #hidden state\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        #out = self.relu(hn)\n",
    "        #out = self.fc_1(out) #first Dense\n",
    "        #out = self.relu(out) #relu\n",
    "        out = self.fc(hn) #Final Output\n",
    "        return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e02d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = 1 # 541 #number of features\n",
    "hidden_size = 50 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 4 #number of output classes \n",
    "# when you have label is 1, it actually means that your label is [0 1 0 0]\n",
    "# if you have label 2 then your label is actually [0 0 1 0]\n",
    "# in some problems you might have label = [0.1 0.2 0.5 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05e4c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]) #our lstm class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6cee3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()    # mean-squared error for regression\n",
    "# cross entropy loss is a dual function that implements softmax and cross-entropy\n",
    "# the reason is that, when num_classes is huge (>10000) it is faster to make a dual function instead of calling\n",
    "# nn.LogSoftmax() and torch.nn.NLLLoss() (Negative log-likelihood Loss)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc90ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(589)\n",
      "Epoch: 0, loss: 1.41176, accuracy: 73.349938\n",
      "tensor(214)\n",
      "Epoch: 10, loss: 1.33313, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 20, loss: 1.26960, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 30, loss: 1.04679, accuracy: 26.650062\n",
      "tensor(214)\n",
      "Epoch: 40, loss: 1.07291, accuracy: 26.650062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/rp63g11s5fv1rxj158yxndfm0000gn/T/ipykernel_8047/850182694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#calculates the loss of the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#improve from loss, i.e backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/master/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/master/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# back-propagation is the algorithm that we use to train neural networks\n",
    "# the core idea of BP is dynamic programming = consuming storage (memory) to store intermediate steps\n",
    "# the large batch size -> the more memory you need -> slow computation\n",
    "# when you are an architect, you build a new network, you need a small batch size to debug\n",
    "# in practice, we always use \"minibatch - learnin\" or \"minibatch stochastic gradient descent\"\n",
    "# instead of using the whole dataset, we give the model a minibatch and update the loss\n",
    "# the idea is that, the statistics of a minibatch can be similar with the dataset\n",
    "# for example: if your data is 1000 dogs 1000 cats and 2000 birds, then by picking randomly 100 samples, you can have \n",
    "# 25 25 50 dogs cats birds -> so the model can still learn\n",
    "\n",
    "import random\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    outputs = model(X_test_tensors_final.float())\n",
    "    outputs = torch.nn.functional.log_softmax(outputs, dim=-1)\n",
    "    predictions = torch.argmax(outputs, 1, keepdim=False)\n",
    "    # predictions = torch.randint(0, 3, y_test.size())\n",
    "    n_correct = torch.eq(predictions, y_test).int().sum()\n",
    "    total = y_test.numel()\n",
    "    \n",
    "    print(n_correct)\n",
    "    accuracy = (n_correct.item() / total) * 100\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "for epoch in range(10000):\n",
    "    iterable_bsz = 128\n",
    "\n",
    "    # we select a random 4 elements from the dataset to train the model\n",
    "    batch_list = list()\n",
    "    #for i in range(iterable_bsz):\n",
    "    #    idx = random.randint(0, X_train_tensors_final.size(0) - 1)\n",
    "    #    batch_list.append(idx)\n",
    "    batch_list = torch.randint(0, X_train_tensors_final.size(0), (iterable_bsz,))\n",
    "    # batch_list = torch.LongTensor(batch_list)\n",
    "    sample = X_train_tensors_final.index_select(0, batch_list)\n",
    "    label = y_train.index_select(0, batch_list)\n",
    "    \n",
    "    outputs = model(sample.float()) #forward pass\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    " \n",
    "  # obtain the loss function\n",
    "    loss = criterion(outputs, label)\n",
    " \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    " \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "    if epoch % 10 == 0:\n",
    "        accuracy = evaluate()\n",
    "        print(\"Epoch: %d, loss: %1.5f, accuracy: %2.6f\" % (epoch, loss.item(), accuracy))\n",
    "        \n",
    "# AFTER TRAINING:\n",
    "\n",
    "# store the weights of model to a dictionary \n",
    "model_weights = model.state_dict()\n",
    "\n",
    "torch.save(\"model.pt\", model_weights)\n",
    "\n",
    "# FOR LOADING THE MODEL:\n",
    "# You need to build the model again\n",
    "#model = LSTM1(.....)\n",
    "\n",
    "weights = torch.load(\"model.pt\")\n",
    "model.load_state_dict(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd60fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e3775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6df115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f11485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
